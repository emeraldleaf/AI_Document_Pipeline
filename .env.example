# AI Document Pipeline Configuration
# Copy this file to .env and customize for your environment

# ============================================================================
# Ollama Configuration (for classification)
# ============================================================================
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# ============================================================================
# Document Processing Paths
# ============================================================================
INPUT_FOLDER=./documents/input
OUTPUT_FOLDER=./documents/output
TEMP_FOLDER=./documents/temp

# ============================================================================
# Classification Categories (comma-separated)
# ============================================================================
CATEGORIES=invoices,contracts,reports,correspondence,research,compliance,other

# ============================================================================
# Processing Options
# ============================================================================
MAX_FILE_SIZE_MB=100
PROCESS_SUBDIRECTORIES=true
PRESERVE_ORIGINAL_STRUCTURE=false

# ============================================================================
# Logging
# ============================================================================
LOG_LEVEL=INFO

# ============================================================================
# Database Configuration (Required for Search)
# ============================================================================

# Enable database storage
USE_DATABASE=true

# Database connection URL
# POC (Local): Use PostgreSQL with Docker
DATABASE_URL=postgresql://docuser:devpassword@localhost:5432/documents

# Production: Use cloud database
# DATABASE_URL=postgresql://user:password@your-rds-endpoint.amazonaws.com:5432/documents

# Store full document content for search (recommended)
STORE_FULL_CONTENT=true

# ============================================================================
# Embedding Configuration (for Semantic Search)
# ============================================================================

# Embedding provider: "ollama" (free, local) or "openai" (paid, cloud)
EMBEDDING_PROVIDER=ollama

# Ollama embedding configuration (POC - Free)
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSION=768

# OpenAI embedding configuration (Production - Paid)
# Uncomment these to use OpenAI embeddings instead of Ollama
# EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-api-key-here
# EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_DIMENSION=1536

# ============================================================================
# Document Splitting (Optional)
# ============================================================================
SPLIT_DOCUMENTS=none  # Options: none, pages, sections, chunks, smart
CHUNK_SIZE=2000
CHUNK_OVERLAP=200
MIN_SECTION_SIZE=100

# ============================================================================
# Quick Start Guide
# ============================================================================
#
# POC Setup (Free - runs on your machine):
# ----------------------------------------
# 1. docker-compose up -d
# 2. ollama pull nomic-embed-text
# 3. pip install -r requirements.txt
# 4. Copy this file to .env (use settings above)
# 5. doc-classify search "test"
#
# Production Setup (Cloud):
# -------------------------
# 1. Create cloud PostgreSQL database (AWS RDS, GCP Cloud SQL, etc.)
# 2. Enable pgvector extension
# 3. Update DATABASE_URL above
# 4. (Optional) Switch EMBEDDING_PROVIDER to openai
# 5. Deploy your application
#
# See SETUP_SEARCH.md for detailed setup instructions
# See CLOUD_MIGRATION.md for production deployment guide
#
