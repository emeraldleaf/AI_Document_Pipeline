# Event-Driven Microservices Architecture

## Overview
Transform the AI Document Pipeline into a modular, event-driven architecture using Docker containers, message queues, and microservices patterns.

---

## Current Architecture (Monolithic)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Application                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Upload   â”‚  â”‚ Search   â”‚  â”‚ Extract  â”‚      â”‚
â”‚  â”‚ Handler  â”‚  â”‚ Handler  â”‚  â”‚ Handler  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚            â”‚            â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚
         â–¼                   â–¼
   PostgreSQL          OpenSearch
```

**Issues:**
- âŒ Tight coupling between services
- âŒ Single point of failure
- âŒ Difficult to scale individual components
- âŒ Can't deploy services independently

---

## Proposed Event-Driven Architecture

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   API Gateway   â”‚
                        â”‚   (Kong/Nginx)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            â”‚            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Ingestion  â”‚  â”‚  Search   â”‚  â”‚  Metadata   â”‚
         â”‚  Service    â”‚  â”‚  Service  â”‚  â”‚  Service    â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚                â”‚                â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Event Bus (RabbitMQ)  â”‚
                    â”‚   or Redis Streams      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            â”‚            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚Classificationâ”‚  â”‚Extraction â”‚  â”‚ Indexing   â”‚
         â”‚  Worker     â”‚  â”‚  Worker   â”‚  â”‚  Worker    â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚                â”‚                â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Storage Layer         â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ PostgreSQL â”‚ OpenSearch â”‚
                    â”‚ S3/MinIO   â”‚ Redis      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Microservices Breakdown

### 1. **API Gateway Service** ðŸŒ
**Responsibility:** Single entry point, routing, authentication, rate limiting

**Tech Stack:**
- Kong or Nginx
- JWT authentication
- Rate limiting per client

**Container:** `api-gateway`

**Events Consumed:** None (HTTP only)
**Events Published:** None

---

### 2. **Ingestion Service** ðŸ“¤
**Responsibility:** Handle file uploads, validate, store, trigger processing

**Tech Stack:**
- FastAPI
- S3 client (MinIO/AWS SDK)
- File validation

**Container:** `ingestion-service`

**API Endpoints:**
- `POST /upload` - Single file upload
- `POST /batch-upload` - Multiple files upload
- `GET /upload-status/{id}` - Check upload status

**Events Published:**
- `document.uploaded` - When file is stored
- `batch.uploaded` - When batch is ready

**Example Event:**
```json
{
  "event_type": "document.uploaded",
  "timestamp": "2025-11-12T10:30:00Z",
  "document_id": "uuid",
  "file_path": "s3://bucket/path/file.pdf",
  "metadata": {
    "filename": "invoice.pdf",
    "size_bytes": 102400,
    "mime_type": "application/pdf"
  }
}
```

---

### 3. **Classification Worker** ðŸ·ï¸
**Responsibility:** Classify documents into categories

**Tech Stack:**
- Python worker process
- Ollama client
- Celery (optional)

**Container:** `classification-worker`

**Events Consumed:**
- `document.uploaded`

**Events Published:**
- `document.classified`

**Processing Logic:**
```python
def classify_document(event):
    document_id = event['document_id']
    file_path = event['file_path']

    # Download from S3
    file_data = s3_client.download(file_path)

    # Classify using LLM
    category = ollama_classifier.classify(file_data)

    # Publish result
    publish_event('document.classified', {
        'document_id': document_id,
        'category': category,
        'confidence': 0.95
    })
```

---

### 4. **Extraction Worker** ðŸ“
**Responsibility:** Extract structured metadata from documents

**Tech Stack:**
- Python worker
- Docling
- LLM metadata extractor

**Container:** `extraction-worker`

**Events Consumed:**
- `document.classified`

**Events Published:**
- `document.extracted`

**Processing Logic:**
```python
def extract_metadata(event):
    document_id = event['document_id']
    category = event['category']

    # Get schema for category
    schema = schema_registry.get(category)

    # Extract using Docling + LLM
    metadata = extractor.extract(file_path, schema)

    # Publish result
    publish_event('document.extracted', {
        'document_id': document_id,
        'metadata': metadata,
        'confidence': 0.87
    })
```

---

### 5. **Indexing Worker** ðŸ”
**Responsibility:** Generate embeddings and index to OpenSearch

**Tech Stack:**
- Python worker
- Ollama embeddings
- OpenSearch client

**Container:** `indexing-worker`

**Events Consumed:**
- `document.extracted`

**Events Published:**
- `document.indexed`

**Processing Logic:**
```python
def index_document(event):
    document_id = event['document_id']
    metadata = event['metadata']

    # Generate embeddings
    embeddings = ollama.embed(metadata['text'])

    # Index to OpenSearch
    opensearch_client.index(
        index='documents',
        id=document_id,
        body={
            'metadata': metadata,
            'embedding': embeddings
        }
    )

    # Publish completion
    publish_event('document.indexed', {
        'document_id': document_id
    })
```

---

### 6. **Search Service** ðŸ”Ž
**Responsibility:** Provide search API

**Tech Stack:**
- FastAPI
- OpenSearch client
- PostgreSQL client

**Container:** `search-service`

**API Endpoints:**
- `GET /search?q=...&mode=...`
- `GET /documents/{id}`

**Events Consumed:** None
**Events Published:** None

---

### 7. **Metadata Service** ðŸ“Š
**Responsibility:** Store and retrieve document metadata

**Tech Stack:**
- FastAPI
- PostgreSQL
- Redis cache

**Container:** `metadata-service`

**API Endpoints:**
- `GET /documents/{id}/metadata`
- `PUT /documents/{id}/metadata`
- `GET /stats`

**Events Consumed:**
- `document.indexed` (to update status)

**Events Published:** None

---

### 8. **Notification Service** ðŸ“¬
**Responsibility:** Send notifications via WebSocket, email, webhooks

**Tech Stack:**
- FastAPI WebSocket
- Redis pub/sub
- Email client (optional)

**Container:** `notification-service`

**Events Consumed:**
- `document.classified`
- `document.extracted`
- `document.indexed`
- `batch.completed`

**Events Published:** None (sends notifications)

**WebSocket Example:**
```python
@app.websocket("/ws/progress/{batch_id}")
async def progress_websocket(websocket: WebSocket, batch_id: str):
    await websocket.accept()

    # Subscribe to Redis channel
    pubsub = redis_client.pubsub()
    pubsub.subscribe(f'batch:{batch_id}')

    # Stream progress updates
    async for message in pubsub.listen():
        await websocket.send_json(message['data'])
```

---

## Event Bus Implementation

### Option 1: RabbitMQ (Recommended)
**Pros:**
- âœ… Mature, battle-tested
- âœ… Rich routing capabilities (topics, fanout, direct)
- âœ… Built-in dead-letter queues
- âœ… Message persistence
- âœ… Management UI

**Cons:**
- âŒ Additional complexity
- âŒ Higher memory usage

**Docker Compose:**
```yaml
rabbitmq:
  image: rabbitmq:3-management
  ports:
    - "5672:5672"    # AMQP
    - "15672:15672"  # Management UI
  environment:
    RABBITMQ_DEFAULT_USER: admin
    RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
  volumes:
    - rabbitmq_data:/var/lib/rabbitmq
```

**Python Client:**
```python
import pika

# Publisher
connection = pika.BlockingConnection(pika.ConnectionParameters('rabbitmq'))
channel = connection.channel()
channel.queue_declare(queue='document.uploaded', durable=True)

channel.basic_publish(
    exchange='',
    routing_key='document.uploaded',
    body=json.dumps(event_data),
    properties=pika.BasicProperties(delivery_mode=2)  # Persistent
)

# Consumer
def callback(ch, method, properties, body):
    event = json.loads(body)
    process_event(event)
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(queue='document.uploaded', on_message_callback=callback)
channel.start_consuming()
```

### Option 2: Redis Streams
**Pros:**
- âœ… Simple, familiar (already using Redis)
- âœ… Lower resource usage
- âœ… Built-in persistence
- âœ… Consumer groups

**Cons:**
- âŒ Less mature for message queuing
- âŒ Simpler routing

**Python Client:**
```python
import redis

r = redis.Redis(host='redis', port=6379)

# Publisher
r.xadd('document.uploaded', {'data': json.dumps(event_data)})

# Consumer
while True:
    events = r.xreadgroup(
        groupname='classification-workers',
        consumername='worker-1',
        streams={'document.uploaded': '>'},
        count=10,
        block=5000
    )
    for stream, messages in events:
        for message_id, data in messages:
            process_event(json.loads(data[b'data']))
            r.xack('document.uploaded', 'classification-workers', message_id)
```

---

## Docker Container Setup

### Directory Structure
```
AI_Document_Pipeline/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.prod.yml
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api-gateway/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ nginx.conf
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ classification-worker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ worker.py
â”‚   â”œâ”€â”€ extraction-worker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ worker.py
â”‚   â”œâ”€â”€ indexing-worker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ worker.py
â”‚   â”œâ”€â”€ search-service/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ metadata-service/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ notification-service/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ main.py
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ publisher.py
â”‚   â”‚   â””â”€â”€ consumer.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ s3_client.py
â””â”€â”€ frontend/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ ...
```

### Docker Compose Configuration

```yaml
version: '3.8'

services:
  # Infrastructure Services
  rabbitmq:
    image: rabbitmq:3-management
    container_name: rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-password}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - doc-pipeline
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 30s
      timeout: 10s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - doc-pipeline
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-documents}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - doc-pipeline
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5

  opensearch:
    image: opensearchproject/opensearch:2
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - DISABLE_SECURITY_PLUGIN=true
    ports:
      - "9200:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    networks:
      - doc-pipeline
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    volumes:
      - minio_data:/data
    networks:
      - doc-pipeline
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 5

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - doc-pipeline

  # Application Services
  api-gateway:
    build: ./services/api-gateway
    container_name: api-gateway
    ports:
      - "80:80"
    depends_on:
      - ingestion-service
      - search-service
      - metadata-service
    networks:
      - doc-pipeline
    volumes:
      - ./services/api-gateway/nginx.conf:/etc/nginx/nginx.conf:ro

  ingestion-service:
    build: ./services/ingestion
    container_name: ingestion-service
    environment:
      RABBITMQ_URL: amqp://admin:${RABBITMQ_PASSWORD:-password}@rabbitmq:5672/
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      POSTGRES_URL: postgresql://postgres:${POSTGRES_PASSWORD:-password}@postgres:5432/documents
    depends_on:
      rabbitmq:
        condition: service_healthy
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - doc-pipeline
    volumes:
      - ./shared:/app/shared:ro

  classification-worker:
    build: ./services/classification-worker
    container_name: classification-worker
    deploy:
      replicas: 3  # Scale as needed
    environment:
      RABBITMQ_URL: amqp://admin:${RABBITMQ_PASSWORD:-password}@rabbitmq:5672/
      OLLAMA_URL: http://ollama:11434
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
    depends_on:
      rabbitmq:
        condition: service_healthy
      ollama:
        condition: service_started
    networks:
      - doc-pipeline
    volumes:
      - ./shared:/app/shared:ro

  extraction-worker:
    build: ./services/extraction-worker
    container_name: extraction-worker
    deploy:
      replicas: 3
    environment:
      RABBITMQ_URL: amqp://admin:${RABBITMQ_PASSWORD:-password}@rabbitmq:5672/
      OLLAMA_URL: http://ollama:11434
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
    depends_on:
      rabbitmq:
        condition: service_healthy
      ollama:
        condition: service_started
    networks:
      - doc-pipeline
    volumes:
      - ./shared:/app/shared:ro

  indexing-worker:
    build: ./services/indexing-worker
    container_name: indexing-worker
    deploy:
      replicas: 2
    environment:
      RABBITMQ_URL: amqp://admin:${RABBITMQ_PASSWORD:-password}@rabbitmq:5672/
      OLLAMA_URL: http://ollama:11434
      OPENSEARCH_URL: http://opensearch:9200
      POSTGRES_URL: postgresql://postgres:${POSTGRES_PASSWORD:-password}@postgres:5432/documents
    depends_on:
      rabbitmq:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - doc-pipeline
    volumes:
      - ./shared:/app/shared:ro

  search-service:
    build: ./services/search-service
    container_name: search-service
    environment:
      OPENSEARCH_URL: http://opensearch:9200
      POSTGRES_URL: postgresql://postgres:${POSTGRES_PASSWORD:-password}@postgres:5432/documents
      REDIS_URL: redis://redis:6379/0
    depends_on:
      opensearch:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - doc-pipeline

  metadata-service:
    build: ./services/metadata-service
    container_name: metadata-service
    environment:
      POSTGRES_URL: postgresql://postgres:${POSTGRES_PASSWORD:-password}@postgres:5432/documents
      REDIS_URL: redis://redis:6379/0
      RABBITMQ_URL: amqp://admin:${RABBITMQ_PASSWORD:-password}@rabbitmq:5672/
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - doc-pipeline

  notification-service:
    build: ./services/notification-service
    container_name: notification-service
    environment:
      REDIS_URL: redis://redis:6379/0
      RABBITMQ_URL: amqp://admin:${RABBITMQ_PASSWORD:-password}@rabbitmq:5672/
    depends_on:
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - doc-pipeline

  frontend:
    build: ./frontend
    container_name: frontend
    ports:
      - "3000:80"
    depends_on:
      - api-gateway
    networks:
      - doc-pipeline

volumes:
  rabbitmq_data:
  redis_data:
  postgres_data:
  opensearch_data:
  minio_data:
  ollama_data:

networks:
  doc-pipeline:
    driver: bridge
```

---

## Shared Event Library

### `shared/events/publisher.py`
```python
import json
from typing import Dict, Any
import pika
from datetime import datetime

class EventPublisher:
    def __init__(self, rabbitmq_url: str):
        self.connection = pika.BlockingConnection(
            pika.URLParameters(rabbitmq_url)
        )
        self.channel = self.connection.channel()

        # Declare exchanges
        self.channel.exchange_declare(
            exchange='documents',
            exchange_type='topic',
            durable=True
        )

    def publish(self, event_type: str, payload: Dict[str, Any]):
        """
        Publish an event to the message bus.

        Args:
            event_type: e.g., 'document.uploaded', 'document.classified'
            payload: Event data
        """
        event = {
            'event_type': event_type,
            'timestamp': datetime.utcnow().isoformat(),
            'payload': payload
        }

        self.channel.basic_publish(
            exchange='documents',
            routing_key=event_type,
            body=json.dumps(event),
            properties=pika.BasicProperties(
                delivery_mode=2,  # Persistent
                content_type='application/json'
            )
        )

        print(f"Published event: {event_type}")

    def close(self):
        self.connection.close()
```

### `shared/events/consumer.py`
```python
import json
from typing import Callable, Dict
import pika

class EventConsumer:
    def __init__(self, rabbitmq_url: str, queue_name: str, event_patterns: list):
        """
        Args:
            queue_name: Unique queue name for this consumer
            event_patterns: List of routing keys to subscribe to (e.g., ['document.uploaded'])
        """
        self.connection = pika.BlockingConnection(
            pika.URLParameters(rabbitmq_url)
        )
        self.channel = self.connection.channel()

        # Declare exchange
        self.channel.exchange_declare(
            exchange='documents',
            exchange_type='topic',
            durable=True
        )

        # Declare queue
        self.channel.queue_declare(queue=queue_name, durable=True)

        # Bind queue to patterns
        for pattern in event_patterns:
            self.channel.queue_bind(
                exchange='documents',
                queue=queue_name,
                routing_key=pattern
            )

        self.queue_name = queue_name
        self.handlers: Dict[str, Callable] = {}

    def register_handler(self, event_type: str, handler: Callable):
        """Register a handler function for an event type."""
        self.handlers[event_type] = handler

    def start(self):
        """Start consuming events."""
        def callback(ch, method, properties, body):
            event = json.loads(body)
            event_type = event['event_type']

            if event_type in self.handlers:
                try:
                    self.handlers[event_type](event['payload'])
                    ch.basic_ack(delivery_tag=method.delivery_tag)
                except Exception as e:
                    print(f"Error processing event: {e}")
                    ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
            else:
                print(f"No handler for event type: {event_type}")
                ch.basic_ack(delivery_tag=method.delivery_tag)

        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=callback
        )

        print(f"Started consuming from queue: {self.queue_name}")
        self.channel.start_consuming()
```

---

## Event Flow Example

### Document Upload Flow
```
1. User uploads document
   â†“
2. API Gateway â†’ Ingestion Service
   â†“
3. Ingestion Service:
   - Validates file
   - Stores to MinIO
   - Saves metadata to PostgreSQL
   - Publishes "document.uploaded" event
   â†“
4. RabbitMQ receives event
   â†“
5. Classification Worker consumes event:
   - Downloads file from MinIO
   - Classifies using Ollama
   - Publishes "document.classified" event
   â†“
6. Extraction Worker consumes event:
   - Downloads file from MinIO
   - Extracts metadata with Docling + LLM
   - Publishes "document.extracted" event
   â†“
7. Indexing Worker consumes event:
   - Generates embeddings
   - Indexes to OpenSearch
   - Updates PostgreSQL status
   - Publishes "document.indexed" event
   â†“
8. Notification Service consumes event:
   - Sends WebSocket notification to client
   - "Document processing complete!"
```

---

## Benefits of Event-Driven Architecture

### 1. **Loose Coupling** ðŸ”—
- Services don't need to know about each other
- Add/remove services without affecting others
- Easy to test services in isolation

### 2. **Scalability** ðŸ“ˆ
- Scale workers independently based on queue depth
- Add more classification workers if classification is slow
- Auto-scaling based on metrics

### 3. **Resilience** ðŸ›¡ï¸
- If a worker crashes, messages stay in queue
- Automatic retry with dead-letter queues
- No data loss

### 4. **Flexibility** ðŸ”„
- Easy to add new processing steps
- A/B testing of different algorithms
- Replay events for reprocessing

### 5. **Observability** ðŸ‘ï¸
- Track events through the system
- Measure processing time per stage
- Identify bottlenecks

---

## Migration Strategy

### Phase 1: Infrastructure Setup (Week 1)
1. âœ… Set up RabbitMQ/Redis Streams
2. âœ… Set up MinIO for file storage
3. âœ… Create shared event library
4. âœ… Update docker-compose.yml

### Phase 2: Extract Workers (Week 2)
1. âœ… Create classification-worker container
2. âœ… Create extraction-worker container
3. âœ… Create indexing-worker container
4. âœ… Keep API as event publisher (hybrid mode)

### Phase 3: Split Services (Week 3)
1. âœ… Extract ingestion logic to ingestion-service
2. âœ… Extract search logic to search-service
3. âœ… Extract metadata logic to metadata-service
4. âœ… Add API gateway

### Phase 4: Add Advanced Features (Week 4)
1. âœ… Add notification-service
2. âœ… Implement dead-letter queues
3. âœ… Add monitoring and metrics
4. âœ… Add health checks

### Phase 5: Production Hardening (Week 5)
1. âœ… Add authentication/authorization
2. âœ… Implement rate limiting
3. âœ… Add distributed tracing
4. âœ… Performance testing and optimization

---

## Monitoring & Observability

### Prometheus Metrics
```yaml
# Add to docker-compose.yml
prometheus:
  image: prom/prometheus
  ports:
    - "9090:9090"
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus

grafana:
  image: grafana/grafana
  ports:
    - "3001:3000"
  environment:
    GF_SECURITY_ADMIN_PASSWORD: admin
  volumes:
    - grafana_data:/var/lib/grafana
```

### Metrics to Track
- Queue depth per event type
- Processing time per worker type
- Success/failure rates
- Document throughput (docs/hour)
- Worker CPU/memory usage
- API response times

---

## Kubernetes Deployment (Future)

### Helm Chart Structure
```
helm/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml
â””â”€â”€ templates/
    â”œâ”€â”€ ingestion-deployment.yaml
    â”œâ”€â”€ classification-deployment.yaml
    â”œâ”€â”€ extraction-deployment.yaml
    â”œâ”€â”€ indexing-deployment.yaml
    â”œâ”€â”€ search-deployment.yaml
    â”œâ”€â”€ metadata-deployment.yaml
    â”œâ”€â”€ notification-deployment.yaml
    â”œâ”€â”€ hpa.yaml  # Horizontal Pod Autoscaler
    â””â”€â”€ service.yaml
```

### Auto-scaling Example
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: classification-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: classification-worker
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: External
    external:
      metric:
        name: rabbitmq_queue_messages
        selector:
          matchLabels:
            queue: document.uploaded
      target:
        type: AverageValue
        averageValue: "30"  # Scale up if >30 messages per pod
```

---

## Conclusion

This event-driven microservices architecture provides:
- âœ… **Modularity**: Each service has a single responsibility
- âœ… **Scalability**: Scale components independently
- âœ… **Resilience**: Fault isolation and automatic retry
- âœ… **Maintainability**: Easy to update individual services
- âœ… **Observability**: Track events through the system
- âœ… **Flexibility**: Add new features without breaking existing ones

**Next Steps:** Start with Phase 1 infrastructure setup!
