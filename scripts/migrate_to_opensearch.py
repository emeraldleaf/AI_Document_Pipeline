#!/usr/bin/env python3
"""
==============================================================================
POSTGRESQL TO OPENSEARCH MIGRATION UTILITY
==============================================================================

PURPOSE:
    Migrate existing documents from PostgreSQL to OpenSearch for
    enterprise-scale search performance.

WHAT IT DOES:
    1. Reads all documents from PostgreSQL database
    2. Creates OpenSearch index with proper mappings
    3. Bulk indexes documents to OpenSearch
    4. Verifies migration success
    5. Optionally generates embeddings if missing

WHEN TO USE:
    - You have documents in PostgreSQL
    - Want to migrate to OpenSearch for better performance
    - Adding OpenSearch to existing deployment
    - Testing OpenSearch without losing PostgreSQL data

SAFETY:
    - READ-ONLY on PostgreSQL (doesn't modify source data)
    - Can be run multiple times safely
    - Existing OpenSearch index will be recreated (if force_recreate=True)

USAGE:
    # Basic migration (preserve existing embeddings)
    python scripts/migrate_to_opensearch.py

    # Regenerate all embeddings
    python scripts/migrate_to_opensearch.py --regenerate-embeddings

    # Migrate to specific index
    python scripts/migrate_to_opensearch.py --index my_docs

    # Dry run (don't actually index)
    python scripts/migrate_to_opensearch.py --dry-run

EXAMPLE:
    $ python scripts/migrate_to_opensearch.py

    üîç PostgreSQL to OpenSearch Migration
    =====================================

    PostgreSQL: postgresql://localhost:5432/documents
    OpenSearch: http://localhost:9200

    üìä Found 50,000 documents in PostgreSQL
    ‚úì Created OpenSearch index: documents
    ‚è≥ Migrating documents...
    ‚úì Migrated 50,000/50,000 documents (100%)
    ‚úì Verified: 50,000 documents in OpenSearch

    üéâ Migration complete!
       Success: 50,000
       Failed: 0
       Duration: 45.2 seconds

PERFORMANCE:
    - ~1,000 documents/second (with embeddings)
    - ~5,000 documents/second (without embeddings)
    - 500K documents in ~8-10 minutes

RELATED FILES:
    - src/opensearch_service.py - OpenSearch operations
    - src/database.py - PostgreSQL operations
    - config.py - Database and OpenSearch configuration

AUTHOR: AI Document Pipeline Team
LAST UPDATED: November 2025
"""

import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Any

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import click
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn
from rich.table import Table
from sqlalchemy import create_engine, text
from loguru import logger

from config import settings
from src.opensearch_service import OpenSearchService
from src.embedding_service import EmbeddingService, EmbeddingProvider

console = Console()


def fetch_documents_from_postgres(
    database_url: str,
    batch_size: int = 1000
) -> List[Dict[str, Any]]:
    """
    Fetch all documents from PostgreSQL.

    Args:
        database_url: PostgreSQL connection URL
        batch_size: Documents to fetch per query

    Returns:
        List of document dictionaries
    """
    try:
        engine = create_engine(database_url)
        documents = []

        with engine.connect() as conn:
            # Get total count
            result = conn.execute(text("SELECT COUNT(*) FROM documents"))
            total = result.fetchone()[0]

            console.print(f"üìä Found {total:,} documents in PostgreSQL\n")

            # Fetch in batches
            offset = 0
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                console=console
            ) as progress:
                task = progress.add_task("Fetching documents...", total=total)

                while offset < total:
                    sql = text("""
                        SELECT
                            id, file_name, file_path, category,
                            title, author, page_count, file_type, file_size,
                            created_date, modified_date, processed_date,
                            full_content, content_preview,
                            confidence,
                            embedding, metadata_json
                        FROM documents
                        ORDER BY id
                        LIMIT :limit OFFSET :offset
                    """)

                    result = conn.execute(sql, {"limit": batch_size, "offset": offset})
                    rows = result.fetchall()

                    for row in rows:
                        # Parse embedding from pgvector string format to list
                        embedding = row[15]
                        if embedding and isinstance(embedding, str):
                            # pgvector returns embeddings as string like "[0.1,0.2,0.3,...]"
                            try:
                                embedding = json.loads(embedding)
                            except (json.JSONDecodeError, ValueError):
                                # If it fails, try parsing as Python literal
                                embedding = None

                        # Parse confidence - convert string to None if not a number
                        confidence = row[14]
                        if confidence and isinstance(confidence, str):
                            try:
                                confidence = float(confidence)
                            except (ValueError, TypeError):
                                confidence = None  # Skip non-numeric confidence values

                        # Convert row to dictionary
                        doc = {
                            "id": row[0],
                            "file_name": row[1],
                            "file_path": row[2],
                            "category": row[3],
                            "title": row[4],
                            "author": row[5],
                            "page_count": row[6],
                            "file_type": row[7],
                            "file_size": row[8],
                            "created_date": row[9],
                            "modified_date": row[10],
                            "processed_date": row[11],
                            "full_content": row[12],
                            "content_preview": row[13],
                            "confidence": confidence,  # Now properly parsed as float or None
                            "embedding": embedding,  # Now properly parsed as list
                            "metadata_json": row[16]  # May be None
                        }
                        documents.append(doc)

                    offset += batch_size
                    progress.update(task, advance=len(rows))

        return documents

    except Exception as e:
        console.print(f"[red]‚ùå Failed to fetch documents from PostgreSQL: {e}[/red]")
        raise


def migrate_documents(
    documents: List[Dict[str, Any]],
    opensearch_service: OpenSearchService,
    index_name: str,
    regenerate_embeddings: bool = False,
    dry_run: bool = False
) -> Dict[str, int]:
    """
    Migrate documents to OpenSearch.

    Args:
        documents: List of documents from PostgreSQL
        opensearch_service: OpenSearch service instance
        index_name: Target index name
        regenerate_embeddings: Regenerate all embeddings
        dry_run: Don't actually index (testing)

    Returns:
        Migration statistics
    """
    try:
        if dry_run:
            console.print("[yellow]üèÉ DRY RUN MODE - No documents will be indexed[/yellow]\n")
            return {"success": 0, "failed": 0, "total": len(documents)}

        console.print("‚è≥ Migrating documents to OpenSearch...\n")

        # If regenerating embeddings, clear existing ones
        if regenerate_embeddings:
            console.print("[yellow]‚ôªÔ∏è  Regenerating embeddings for all documents[/yellow]")
            for doc in documents:
                doc["embedding"] = None  # Will be regenerated

        # Bulk index
        result = opensearch_service.bulk_index_documents(
            index_name=index_name,
            documents=documents,
            generate_embeddings=True,  # Generate if missing
            chunk_size=500
        )

        return result

    except Exception as e:
        console.print(f"[red]‚ùå Migration failed: {e}[/red]")
        raise


def verify_migration(
    opensearch_service: OpenSearchService,
    index_name: str,
    expected_count: int
) -> bool:
    """
    Verify migration success.

    Args:
        opensearch_service: OpenSearch service
        index_name: Index to verify
        expected_count: Expected document count

    Returns:
        True if verification passed
    """
    try:
        # Refresh index to make documents searchable
        opensearch_service.refresh_index(index_name)

        # Get index stats
        stats = opensearch_service.get_index_stats(index_name)
        actual_count = stats.get("document_count", 0)

        if actual_count == expected_count:
            console.print(f"[green]‚úì Verified: {actual_count:,} documents in OpenSearch[/green]")
            return True
        else:
            console.print(f"[yellow]‚ö†Ô∏è  Count mismatch: Expected {expected_count:,}, found {actual_count:,}[/yellow]")
            return False

    except Exception as e:
        console.print(f"[red]‚ùå Verification failed: {e}[/red]")
        return False


@click.command()
@click.option(
    "--database-url",
    default=None,
    help="PostgreSQL connection URL (default: from config)"
)
@click.option(
    "--opensearch-hosts",
    default="http://localhost:9200",
    help="OpenSearch hosts (default: http://localhost:9200)"
)
@click.option(
    "--index",
    default="documents",
    help="OpenSearch index name (default: documents)"
)
@click.option(
    "--regenerate-embeddings",
    is_flag=True,
    help="Regenerate all embeddings (slow but ensures consistency)"
)
@click.option(
    "--force-recreate",
    is_flag=True,
    help="Delete and recreate index (‚ö†Ô∏è destroys existing data)"
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Test migration without actually indexing"
)
@click.option(
    "--batch-size",
    default=1000,
    help="Documents to fetch per batch (default: 1000)"
)
def main(
    database_url: str,
    opensearch_hosts: str,
    index: str,
    regenerate_embeddings: bool,
    force_recreate: bool,
    dry_run: bool,
    batch_size: int
):
    """
    Migrate documents from PostgreSQL to OpenSearch.

    This script safely copies your document index from PostgreSQL to OpenSearch
    for improved search performance at scale (500K+ documents).

    Examples:

        # Basic migration
        python scripts/migrate_to_opensearch.py

        # Regenerate all embeddings
        python scripts/migrate_to_opensearch.py --regenerate-embeddings

        # Test without indexing
        python scripts/migrate_to_opensearch.py --dry-run
    """
    start_time = time.time()

    # Print header
    console.print("\n[bold cyan]üîç PostgreSQL to OpenSearch Migration[/bold cyan]")
    console.print("=" * 50)
    console.print()

    # Use config database_url if not provided
    db_url = database_url or settings.database_url

    console.print(f"[dim]PostgreSQL: {db_url.split('@')[1] if '@' in db_url else db_url}[/dim]")
    console.print(f"[dim]OpenSearch: {opensearch_hosts}[/dim]")
    console.print(f"[dim]Index: {index}[/dim]")
    console.print()

    try:
        # Initialize services
        embedding_service = EmbeddingService.create(
            provider=EmbeddingProvider(settings.embedding_provider)
        )

        opensearch_service = OpenSearchService(
            hosts=[opensearch_hosts],
            embedding_service=embedding_service
        )

        # Check OpenSearch health
        health = opensearch_service.health_check()
        if health.get("status") not in ["green", "yellow"]:
            console.print(f"[red]‚ùå OpenSearch is unhealthy: {health}[/red]")
            return

        console.print(f"[green]‚úì OpenSearch cluster: {health.get('cluster_name')} ({health.get('status')})[/green]")
        console.print()

        # Create index
        if not dry_run:
            success = opensearch_service.create_index(
                index_name=index,
                dimension=settings.embedding_dimension,
                force_recreate=force_recreate
            )

            if success:
                console.print(f"[green]‚úì Created OpenSearch index: {index}[/green]")
            else:
                console.print(f"[yellow]‚ö†Ô∏è  Index already exists: {index}[/yellow]")

            console.print()

        # Fetch documents from PostgreSQL
        documents = fetch_documents_from_postgres(db_url, batch_size)

        if not documents:
            console.print("[yellow]‚ö†Ô∏è  No documents found in PostgreSQL[/yellow]")
            return

        # Migrate documents
        result = migrate_documents(
            documents=documents,
            opensearch_service=opensearch_service,
            index_name=index,
            regenerate_embeddings=regenerate_embeddings,
            dry_run=dry_run
        )

        # Print results
        console.print()
        console.print("[bold green]üéâ Migration complete![/bold green]\n")

        # Results table
        table = Table(show_header=True, header_style="bold cyan")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green", justify="right")

        table.add_row("Total documents", f"{result['total']:,}")
        table.add_row("Successful", f"{result['success']:,}")
        table.add_row("Failed", f"{result['failed']:,}")
        table.add_row("Success rate", f"{(result['success'] / result['total'] * 100):.1f}%")
        table.add_row("Duration", f"{time.time() - start_time:.1f} seconds")

        console.print(table)
        console.print()

        # Verify migration
        if not dry_run and result['success'] > 0:
            time.sleep(2)  # Wait for indexing to complete
            verify_migration(opensearch_service, index, result['success'])

        console.print()
        console.print("[dim]Next steps:[/dim]")
        console.print("[dim]  1. Update .env: SEARCH_BACKEND=opensearch[/dim]")
        console.print("[dim]  2. Test search: doc-classify search 'your query'[/dim]")
        console.print("[dim]  3. View dashboard: http://localhost:5601[/dim]")
        console.print()

    except Exception as e:
        console.print(f"\n[red]‚ùå Migration failed: {e}[/red]")
        import traceback
        console.print(f"[dim]{traceback.format_exc()}[/dim]")
        sys.exit(1)


if __name__ == "__main__":
    main()
